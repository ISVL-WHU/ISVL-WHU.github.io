<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG" />
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG" />
  <meta property="og:url" content="URL OF THE WEBSITE" />
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200" />
  <meta property="og:image:height" content="630" />


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Hi-Way</title>
  <link rel="icon" type="image/x-icon" href="static/images/icon.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>

<body>

  <style>
    .video-container {
      width: 30%;
      /* 每个容器宽度为页面宽度的32%，根据实际情况调整 */
      display: inline-block;
      /* 使容器并列排列 */
      margin: 1%;
      /* 添加外边距 */
      vertical-align: top;
    }

    video {
      width: 100%;
      /* 视频宽度填满容器 */
      display: block;
      /* 确保视频占满整个容器宽度 */
    }

    .description {
      text-align: center;
      /* 文字居中对齐 */
    }

    /* 添加网格布局样式 */
    .comparison-grid {
      display: grid;
      grid-template-columns: 1fr 1fr;
      gap: 20px;
      margin-top: 20px;
    }

    .grid-cell {
      text-align: center;
    }

    .grid-cell video {
      width: 100%;
      border-radius: 8px;
    }

    .grid-cell .method-label {
      font-weight: bold;
      font-size: 18px;
      margin-bottom: 10px;
      color: #363636;
    }

    .grid-cell .case-label {
      font-weight: bold;
      font-size: 16px;
      margin-bottom: 10px;
      color: #4a4a4a;
    }

    .text-image-container {
      display: flex;
      /* 使用弹性盒模型布局 */
      align-items: flex-start;
      /* 上端对齐 */
      justify-content: center;
    }

    ul {
      text-align: left;
      /* 确保文本左对齐 */
      list-style-position: inside;
      /* 列表标记与文本对齐 */
      font-size: 16px;
      list-style-type: circle;
      /* 使用圆形作为项目符号 */

    }
  </style>




  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Hi-Way: Hierarchical Framework for Continuous Vision-Language
              Navigation via Map Guidance and Waypoint Reasoning</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <!-- <span class="author-block"><a href="FIRST AUTHOR PERSONAL LINK" target="_blank">Jiazhao Zhang</a><sup>1,2,*</sup>,</span> -->
              <!-- <span class="author-block">
                <a href="https://jzhzhang.github.io/">Jiazhao
                  Zhang</a><sup>1,2,*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                Kunyu Wang<sup>2,*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://scholar.google.com.hk/citations?user=_IUq7ooAAAAJ">Rongtao
                  Xu</a><sup>2,3,*</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://github.com/GengzeZhou">Gengze Zhou</a><sup>4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://yiconghong.me/">Yicong Hong</a><sup>5</sup>
              </span> -->
              <!-- <span class="author-block" style="display: block;">
                Xiaomeng Fang<sup>2</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="http://qi-wu.me/">Qi Wu</a><sup>4</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://scholar.google.com/citations?user=X7M0I8kAAAAJ">Zhizheng
                  Zhang</a><sup>6,†</sup>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <a href="https://hughw19.github.io/">He Wang</a><sup>1,2,†</sup>
              </span> -->
            </div>

            <div class="is-size-5 publication-authors">
              <!-- <span class="author-block">Institution Name<br>Conferance name and year</span> -->
              <!-- <span class="author-block"><sup>1</sup>
                Peking University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>2</sup>Beijing Academy of Artificial Intelligence&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>3</sup>CASIA
              </span>
              <span class="author-block" style="display: block;">
                <sup>4</sup>University of Adelaide&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>5</sup>Australian National University&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
                <sup>6</sup>GalBot
              </span>
              <span class="eql-cntrb" style="display: block;">
                <small><sup>*</sup>Indicates Equal Contribution,&nbsp;</small>
                <small><sup>†</sup>Indicates Equal Advising.</small>
              </span> -->
            </div>




            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- Arxiv PDF link -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>

                <!-- Supplementary PDF link -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-file-video"></i>
                    </span>
                    <span>Video</span>
                  </a>
                </span>

                <!-- Github link -->
                <span class="link-block">
                  <a href="" target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- Hugging Face link -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/zzywhu/VLN_Waypoint/tree/main" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fas fa-rocket"></i>
                    </span>
                    <span>Hugging Face</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
                <!-- <span class="link-block">
                  <a href="https://arxiv.org/abs/2402.15852" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>Paper</span>
                  </a>
                </span> -->
              </div>
            </div>

            <div class="text">
              European Conference on Computer Vision (ECCV 2026)
            </div>

          </div>
        </div>
      </div>
    </div>
  </section>



  <section class="hero is-small">

    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">

        <h2>
          <!-- <h2 class=""> <img src="static/images/highlight_logo.png" width="50"> Highlights</h2> -->
          <div class="text-image-container title is-3">
            <div>
              <img src="static/images/highlight_logo.png" alt="示例图片" width="50">
            </div>
            <div class="text">
              <p>Highlights</p>
            </div>
          </div>
        </h2>



        <!-- <p> <b>Real-world demos by following simple instructions, such as walking to a single landmark.</b></p> -->
        <ul>
          <li><b></b></li>
          <li><b></b></li>
          <li><b></b></li>
          <!-- <li><b>NaVid is co-trained with real-world caption data (763k) and simulated VLN data (510k). The VLN capability is obtained by leveraging simulation environments, while real-world understanding is gained through real-world caption data.
          </b></li> -->
          <li><b></b></li>
        </ul>


      </div>

    </div>

  </section>



  <!-- Teaser video 2 -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Realworld Demos</h2>
        <p style="margin-bottom: 16px;">
          <b>These demos show real-world Hi-Way performance of robot. </b>
        </p>

        <div class="demo-block">
          <p class="has-text-centered" style="margin-bottom: 8px;">
            <b>Instruction:</b> Turn around and go straight forward to the blue box.
          </p>
          <video poster="" id="video10" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/ours.mp4" type="video/mp4">
          </video>
        </div>

        <div class="demo-block">
          <p class="has-text-centered" style="margin-bottom: 8px;">
            <b>Instruction:</b> Turn around and go straight forward to the blue box.
          </p>
          <video poster="" id="video11" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/没看见蓝箱子(ours).mp4" type="video/mp4">
          </video>
        </div>

        <div class="demo-block">
          <p class="has-text-centered" style="margin-bottom: 8px;">
            <b>Instruction:</b> Go straight to the blue box and then turn right to exit the room.
          </p>
          <video poster="" id="video12" autoplay playsinline controls muted loop height="100%">
            <source src="static/videos/出门(ours).mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div>
  </section>
  <!-- End teaser video 2 -->

  <!-- Paper abstract -->
  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              Vision-and-Language Navigation (VLN) stands as a key research problem of Embodied AI, aiming at enabling
              agents to navigate in unseen environments following linguistic instructions. In this field, generalization
              is a long-standing challenge, either to out-of-distribution scenes or from Sim to Real. In this paper, we
              propose NaVid, a video-based large vision language model (VLM), to mitigate such a generalization gap.
              NaVid makes the first endeavour to showcase the capability of VLMs to achieve state-of-the-art level
              navigation performance without any maps, odometer and depth inputs. Following human instruction, NaVid
              only requires an on-the-fly video stream from a monocular RGB camera equipped on the robot to output the
              next-step action. Our formulation mimics how humans navigate and naturally gets rid of the problems
              introduced by odometer noises, and the Sim2Real gaps from map or depth inputs. Moreover, our video-based
              approach can effectively encode the historical observations of robots as spatio-temporal contexts for
              decision making and instruction following. We train NaVid with 510k navigation samples collected from
              VLN-CE trajectories, including action planning and instruction-reasoning samples, along with 763k
              large-scale web data. Extensive experiments show that NaVid achieves SOTA performance in simulation
              environments and the real world, demonstrating superior cross-dataset and Sim2Real transfer. We thus
              believe our proposed VLM approach plans the next step for not only the navigation agents but also this
              research field. We will release the code and data to benefit the community.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>
  <!-- End paper abstract -->

  <!-- YouTube Video-->


  <!-- Method Overview -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content">
              <h2 class="title is-3">Method Overview</h2>
              <img src="static/images/framework.png" alt="NaVid" class="center-image blend-img-background">
              <div class="level-set has-text-justified">
                <p class="has-text-justified">
                  <b>Pipeline of Hi-Way</b>. Our hierarchical framework decomposes
                  the complex VLN task into three controllable modules. (1) <i>High-level
                    planning</i>: an LLM-driven planner decomposes the instruction into an ordered sequence
                  of sub-tasks. (2) <i>Middle-level perception and waypoint reasoning</i>:
                  the robot acquires a surround view via in-place rotation, performs image-text
                  matching to build a interest obstacle map, and selects the optimal waypoint to
                  refine the route. (3) <i>Low-level action execution</i>: a ROS-aligned
                  controller executes waypoint navigation through velocity commands (<i>/cmd_vel</i>).
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered has-text-centered">
          <div class="column is-four-fifths">
            <div class="content">
              <h2 class="title is-3">VLN-Waypoint Dataset Construction</h2>
              <img src="static/images/dataset.png" alt="NaVid" class="center-image blend-img-background">
              <div class="level-set has-text-justified">
                <p class="has-text-justified">
                  <b>VLN-Waypoint dataset construction pipeline.</b> (a) Starting from
                  classic VLN observations (RGB/depth) and multi-step action sequences. (b) An
                  action-to-waypoint policy maps the next four actions to a discrete waypoint
                  label (including special cases such as <i>stop</i>). (c) Using depth-based
                  3D ground segmentation, we sample traversable waypoint candidates and project
                  them into the image to obtain waypoint-annotated views. (d) We package each
                  sample as an instruction-route pair with the current and history frames,
                  together with the projected waypoint candidates, forming our VLN-Waypoint
                  dataset.
                </p>
                <p class="has-text-justified">
                  <!-- <b>We initialize the encoders and Vicuna-7B using pre-trained weights, and our model requires only one epoch for the training process.</b> -->
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>





  <!-- End Method Overview  -->

  <!-- Image carousel Caption  -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
          <h2 class="title is-3">Caption Results Visualization</h2>
          <div id="results-carousel" class="carousel results-carousel">
            <div class="item"><img src="static/images/caption/caption-1.png" alt="MY ALT TEXT"/></div>
            <div class="item"><img src="static/images/caption/caption-2.png" alt="MY ALT TEXT"/></div>
            <div class="item"><img src="static/images/caption/caption-3.png" alt="MY ALT TEXT"/></div>
            <div class="item"><img src="static/images/caption/caption-4.png" alt="MY ALT TEXT"/></div>
            <div class="item"><img src="static/images/caption/caption-5.png" alt="MY ALT TEXT"/></div>
            <div class="item"><img src="static/images/caption/caption-6.png" alt="MY ALT TEXT"/></div>
          </div>
    </div>
  </div>
</section> -->
  <!-- End image carousel Caption  -->

  <!-- Video carousel 1 Caption -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Results of Navigation Video Captioning</h2>
        <p> <b>Comparison between our method and NaVid on visible and non-visible cases. </b></p>

        <div class="comparison-grid">
          <!-- Row 1, Column 1: Our Method - Visible Case -->
          <div class="grid-cell">
            <div class="method-label">Our Method (Hi-Way)</div>
            <div class="case-label">Visible Case</div>
            <img src="./static/gif/visible/our1.gif" alt="Hi-Way Visible Case">
            <div class="description"><b>Hi-Way:</b> Walk forward and turn left. Wait by the first door on the left.
            </div>
          </div>

          <!-- Row 1, Column 2: Our Method - Non-visible Case -->
          <div class="grid-cell">
            <div class="method-label">Our Method (Hi-Way)</div>
            <div class="case-label">Non-visible Case</div>
            <img src="./static/gif/visible/our2.gif" alt="Hi-Way Non-visible Case">
            <div class="description"><b>Hi-Way:</b> Walk forward and turn left. Wait near the first doorway.</div>
          </div>

          <!-- Row 2, Column 1: NaVid - Visible Case -->
          <div class="grid-cell">
            <div class="method-label">NaVid</div>
            <div class="case-label">Visible Case</div>
            <img src="./static/gif/visible/navid1.gif" alt="NaVid Visible Case">
            <div class="description"><b>NaVid:</b> Exit the bathroom. Turn left and enter the bedroom. Wait near the
              bed.</div>
          </div>

          <!-- Row 2, Column 2: NaVid - Non-visible Case -->
          <div class="grid-cell">
            <div class="method-label">NaVid</div>
            <div class="case-label">Non-visible Case</div>
            <img src="./static/gif/visible/navid2.gif" alt="NaVid Non-visible Case">
            <div class="description"><b>NaVid:</b> Walk through the doorway and turn left. Walk past the pool and wait
              by the first chair.</div>
          </div>
        </div>

      </div>
    </div>
  </section>


  <!-- Comparison Section: Single-task vs Multiple-task -->
  <section class="hero is-small">
    <div class="hero-body">
      <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Single-task vs Multiple-task Comparison</h2>
        <p><b>Comparison of different methods on single-task and multiple-task navigation scenarios.</b></p>

        <style>
          .comparison-grid-3col {
            display: grid;
            grid-template-columns: 1fr 1fr 1fr;
            gap: 20px;
            margin-top: 20px;
          }

          .comparison-grid-3col .grid-cell {
            text-align: center;
          }

          .comparison-grid-3col .grid-cell img {
            width: 100%;
            border-radius: 8px;
          }

          .comparison-grid-3col .method-label {
            font-weight: bold;
            font-size: 18px;
            margin-bottom: 10px;
            color: #363636;
          }

          .comparison-grid-3col .task-label {
            font-weight: bold;
            font-size: 16px;
            margin-bottom: 10px;
            color: #4a4a4a;
          }

          .comparison-grid-3col .description {
            text-align: center;
            margin-top: 10px;
            font-size: 14px;
          }
        </style>

        <div class="comparison-grid-3col">
          <!-- Row 1: Single-task -->
          <!-- Column 1: Our Method - Single-task -->
          <div class="grid-cell">
            <div class="method-label">Our Method (Hi-Way)</div>
            <div class="task-label">Single-task</div>
            <img src="./static/gif/goal/our1.gif" alt="Hi-Way Single-task">
            <div class="description"><b>Hi-Way:</b> Turn left and walk to the chair.</div>
          </div>

          <!-- Column 2: NaVid - Single-task -->
          <div class="grid-cell">
            <div class="method-label">NaVid</div>
            <div class="task-label">Single-task</div>
            <img src="./static/gif/goal/navid1.gif" alt="NaVid Single-task">
            <div class="description"><b>NaVid:</b> Turn left and walk to the chair.</div>
          </div>

          <!-- Column 3: Uni-NaVid - Single-task -->
          <div class="grid-cell">
            <div class="method-label">Uni-NaVid</div>
            <div class="task-label">Single-task</div>
            <img src="./static/gif/goal/uni1.gif" alt="Uni-NaVid Single-task">
            <div class="description"><b>Uni-NaVid:</b> Turn left and walk to the chair.</div>
          </div>

          <!-- Row 2: Multiple-task -->
          <!-- Column 1: Our Method - Multiple-task -->
          <div class="grid-cell">
            <div class="method-label">Our Method (Hi-Way)</div>
            <div class="task-label">Multiple-task</div>
            <img src="./static/gif/goal/our2.gif" alt="Hi-Way Multiple-task">
            <div class="description"><b>Hi-Way:</b> Exit the room, turn right, and walk to the kitchen table.</div>
          </div>

          <!-- Column 2: NaVid - Multiple-task -->
          <div class="grid-cell">
            <div class="method-label">NaVid</div>
            <div class="task-label">Multiple-task</div>
            <img src="./static/gif/goal/navid2.gif" alt="NaVid Multiple-task">
            <div class="description"><b>NaVid:</b> Exit the room, turn right, and walk to the kitchen table.</div>
          </div>

          <!-- Column 3: Uni-NaVid - Multiple-task -->
          <div class="grid-cell">
            <div class="method-label">Uni-NaVid</div>
            <div class="task-label">Multiple-task</div>
            <img src="./static/gif/goal/uni2.gif" alt="Uni-NaVid Multiple-task">
            <div class="description"><b>Uni-NaVid:</b> Exit the room, turn right, and walk to the kitchen table.</div>
          </div>
        </div>

      </div>
    </div>
  </section>
  <!-- End Comparison Section -->
  <!-- End Video carousel 1 Caption -->

  <!-- Image carousel 2 R2R -->
  <!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
      <h2 class="title is-3">R2R Data Visualization</h2>
      <div id="results-carousel" class="carousel results-carousel">  
        <div class="item"><img src="static/images/r2r/r2r_1.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img src="static/images/r2r/r2r_2.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img src="static/images/r2r/r2r_3.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img src="static/images/r2r/r2r_4.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img src="static/images/r2r/r2r_5.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img src="static/images/r2r/r2r_6.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img src="static/images/r2r/r2r_7.png" alt="MY ALT TEXT"/></div>
        <div class="item"><img src="static/images/r2r/r2r_8.png" alt="MY ALT TEXT"/></div>
      </div>
    </div>
  </div>
</section> -->
  <!-- End image carousel 2 R2R -->





  <!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code></code></pre>
    </div>
  </section>
  <!--End BibTex citation -->


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">

            <p>
              This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template"
                target="_blank">Academic Project Page Template</a> which was adopted from the <a
                href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
              You are free to borrow the of this website, we just ask that you link back to this page in the footer.
              <br> This website is licensed under a <a rel="license"
                href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
                Commons Attribution-ShareAlike 4.0 International License</a>.
            </p>

          </div>
        </div>
      </div>
    </div>
  </footer>

  <!-- Statcounter tracking code -->

  <!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

  <!-- End of Statcounter Code -->

</body>

</html>